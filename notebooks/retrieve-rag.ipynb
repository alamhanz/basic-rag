{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/llama-2/llama-2-13b-retrievalqa.ipynb\n",
    "from torch import cuda\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, AutoModelForQuestionAnswering\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "#AutoModelForQuestionAnswering\n",
    "\n",
    "# tl_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     trust_remote_code=True,\n",
    "#     # quantization_config=bnb_config,\n",
    "#     device_map='auto',\n",
    "#     # use_auth_token=hf_auth\n",
    "# )\n",
    "\n",
    "# tl_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "\n",
    "langchain_tg = pipeline(\n",
    "    model=model_id,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task= \"text-generation\",\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"auto\",\n",
    "    # we pass model parameters here too\n",
    "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")\n",
    "\n",
    "basic_llm = HuggingFacePipeline(\n",
    "    pipeline=langchain_tg,\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amoza\\anaconda3\\envs\\tprep\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nTINYLLAMA: (smiling) Well, I'm a tiny little lamasaurus. And I have the power to transform into a small, furry animal!\\n\\nJACK: (impressed) That's amazing! Can you show me how it works?\\n\\nTINYLLAMA: (shows him) Sure thing! Whenever I feel happy or excited, I can transform into a rabbit or a squirrel. It's like having a whole new personality!\\n\\nJACK: (laughs) Wow, that's cool! But why did you choose to be a lamasaurus instead of a rabbit or a squirrel?\\n\\nTINYLLAMA: (pauses) Actually, there was a time when I wanted to be a rabbit. But then I realized that being a rabbit would mean living in a cage all day long. So I decided to become a lamasaurus instead. It's a bit more exciting and adventurous, don't you think?\\n\\nJACK: (nodding) Yeah, I guess it is. But what about your friends at school? Do they know about your powers?\\n\\nTINYLLAMA: (smiles) Not really. Most of my classmates are too busy worrying about their own problems to pay much attention to me. But I've been practicing my transformation skills for years now, and I'm pretty good at it.\\n\\nJACK: (surprised) You're really good at it! How do you do it?\\n\\nTINYLLAMA: (shrugs) I don't know exactly. It's just something that comes naturally to me. But I always try to keep it a secret from my classmates. They might think I'm crazy or something.\\n\\nJACK: (chuckles) Crazy or not, you're definitely unique. Thanks for sharing your story with us, Tinyllama!\\n\\nTINYLLAMA: (grinning) You're welcome! I hope we get to see each other again soon. Maybe we could even go on a adventure together someday.\\n\\nJACK: (smiling) Sounds like a plan! Let's do this!\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_llm(prompt=\"what is so special about Tinyllama?\") ## 8 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=embed_model_id,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'device': device, 'batch_size': 32}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vstore = FAISS.load_local(\"../Documents/vdb_faiss_index\", embed_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "rag_llm = RetrievalQA.from_chain_type(\n",
    "    llm=basic_llm, chain_type='stuff',\n",
    "    retriever=vstore.as_retriever()\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amoza\\anaconda3\\envs\\tprep\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. The Tokyo Olympics will be held from July 23 to August 8, 2021.\n",
      "\n",
      "2. The opening ceremony will take place on July 23 at 6:00 pm (JST).\n",
      "\n",
      "3. The closing ceremony will take place on August 8 at 9:00 am (JST).\n",
      "\n",
      "4. The duration of the games is expected to be around 17 days.\n",
      "\n",
      "5. The games will feature a total of 33 sports and 33 disciplines.\n",
      "\n",
      "6. The opening and closing ceremonies will be broadcast live on TV and online.\n",
      "\n",
      "7. The athletes' village will be located in the city of Chiba, about 30 km northwest of Tokyo.\n",
      "\n",
      "8. The venues for the games include the Olympic Stadium, the National Gymnasium, and other smaller venues.\n",
      "\n",
      "9. The games will have a budget of approximately $10 billion.\n",
      "\n",
      "10. The Tokyo Olympics are part of the International Olympic Committee's (IOC) Olympic Agenda 2020, which aims to modernize and improve the Olympic Games.\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "ans1 = basic_llm(prompt=\"what happen in Japan 2024?\")\n",
    "print(ans1) #4 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Japanese Red Cross Society is sending relief teams to help those affected by the massive earthquake that hit Japan on 1 January 2024.\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "ans2 = rag_llm(\"what happen in Japan 2024?\")\n",
    "print(ans2[\"result\"]) ## 8 Minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='2024.01.12\\n\\nThe Japanese Red Cross Society would like to express our sincere condolences\\nand sympathy toward the people affected by the massive earthquake which hit\\nJapan on 1 January 2024. Our relief teams have been working around the clock\\nto save lives, protect health and dignity of the affected people in Noto\\nPeninsula.\\n\\n## 1\\\\. Situation', metadata={'id': 'de1e9727ae72-3', 'source': 'https://www.jrc.or.jp/english/relief/2024NotoPeninsulaEarthquake.html'}),\n",
       " Document(page_content='213. **^** \"Japan govt. supplies quake-hit areas, probes damage to ships\". NHK. 4 January 2024. Archived from the original on 5 January 2024. Retrieved 4 January 2024.', metadata={'id': 'f60ddb659058-425', 'source': 'https://en.wikipedia.org/wiki/2024_Noto_earthquake'}),\n",
       " Document(page_content='209. **^** \"Transportation systems along Sea of Japan still affected by Monday\\'s quake\". NHK. 2 January 2024. Archived from the original on 2 January 2024. Retrieved 2 January 2024.', metadata={'id': 'f60ddb659058-421', 'source': 'https://en.wikipedia.org/wiki/2024_Noto_earthquake'}),\n",
       " Document(page_content='## 1\\\\. Situation\\n\\n\\\\- On 1 January 2024 at 16:10 (Japan Standard Time), a magnitude 7.6\\nearthquake struck the Noto Peninsula in Ishikawa Prefecture, Japan. As the\\nepicenter was very shallow, large tremors were observed in many places and a\\ntsunami warning was issued. Since then, 1,320 aftershocks have followed the\\nmain shock.', metadata={'id': 'de1e9727ae72-4', 'source': 'https://www.jrc.or.jp/english/relief/2024NotoPeninsulaEarthquake.html'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vstore.similarity_search(\"what happen in Japan 2024?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/List_of_earthquakes_in_Japan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amoza\\anaconda3\\envs\\tprep\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Noto Earthquake was a 7.2 magnitude earthquake that occurred on August 16, 2019, in the Pacific Ocean off the coast of Japan. The earthquake caused significant damage to buildings and infrastructure, including the collapse of several buildings in Tokyo. It also triggered tsunami warnings for parts of Japan's coastline. The earthquake was felt throughout much of Asia, including China, Korea, and Taiwan.\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "ans1 = basic_llm(prompt=\"what is Noto Earthquake?\")\n",
    "print(ans1) #1.5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amoza\\anaconda3\\envs\\tprep\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Noto Peninsula earthquake occurred on January 18, 2024,\n",
      "with a magnitude of 6.9 and a depth of 10 km. It caused significant damage to\n",
      "the region, with over 200 fatalities and more than 1,000 injuries.\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "ans2 = rag_llm(\"what is Noto Earthquake?\")\n",
    "print(ans2[\"result\"]) # 6 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amoza\\anaconda3\\envs\\tprep\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "ans1 = basic_llm(prompt=\"List down all year when the Noto Earthquake happen\")\n",
    "print(ans1) #1.5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amoza\\anaconda3\\envs\\tprep\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Noto Earthquake happened in 2024.\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "ans2 = rag_llm(\"List down all year when the Noto Earthquake happen\")\n",
    "print(ans2[\"result\"]) # 8 minutes# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amoza\\anaconda3\\envs\\tprep\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "ans1 = basic_llm(prompt=\"who gives tsunami warning after Noto Earthquake in 2024?\")\n",
    "print(ans1) #1.5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amoza\\anaconda3\\envs\\tprep\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Japan Meteorological Agency (JMA) issues tsunami warnings after earthquakes in Japan.\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "ans2 = rag_llm(\"who gives tsunami warning after Noto Earthquake in 2024?\")\n",
    "print(ans2[\"result\"]) ## 6 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tprep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
